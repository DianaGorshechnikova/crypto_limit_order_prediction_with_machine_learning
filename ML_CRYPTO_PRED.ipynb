{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f525b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coinbase.rest import RESTClient\n",
    "from json import dumps\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import tqdm\n",
    "import asyncio\n",
    "import ccxt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pprint import pprint\n",
    "\n",
    "## WITH VIEW PERMISSIONS ONLY\n",
    "api_key = \"organizations/XXXXXX\"\n",
    "api_secret = \"-----BEGIN EC PRIVATE KEY-----\\XXXXXX\\n-----END EC PRIVATE KEY-----\\n\"\n",
    "\n",
    "client = RESTClient(api_key=api_key, api_secret=api_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db7e56",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8923e877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62215\n"
     ]
    }
   ],
   "source": [
    "product = client.get_product(\"BTC-USD\")\n",
    "btc_usd_price = float(product[\"price\"])\n",
    "adjusted_btc_usd_price = str(math.floor(btc_usd_price - (btc_usd_price * 0.05)))\n",
    "print(adjusted_btc_usd_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6278caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.pro.coinbase.com'\n",
    "\n",
    "# Function to get historical rates\n",
    "def fetch_data_in_chunks(product_id, start, end, granularity):\n",
    "    chunk_size = timedelta(days=300*granularity/86400)  # Calculate the size of each chunk\n",
    "    chunks = []\n",
    "    \n",
    "    current_start = start\n",
    "    while current_start < end:\n",
    "        current_end = min(current_start + chunk_size, end)\n",
    "        url = f'https://api.pro.coinbase.com/products/{product_id}/candles'\n",
    "        params = {\n",
    "            'start': current_start.isoformat(),\n",
    "            'end': current_end.isoformat(),\n",
    "            'granularity': granularity\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            # Convert to DataFrame and append to the list\n",
    "            df = pd.DataFrame(response.json(), columns=['datetime', 'open', 'high', 'low', 'close', 'volume'])\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'], unit='s')\n",
    "            chunks.append(df)\n",
    "        else:\n",
    "            print(f\"Error fetching data: {response.json()}\")\n",
    "        current_start = current_end  # Update the start time for the next chunk\n",
    "    \n",
    "    # Combine all chunks into a single DataFrame\n",
    "    return pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Function to get market orders\n",
    "def get_market_orders(product_id):\n",
    "    trades_url = f'{base_url}/products/{product_id}/trades'\n",
    "    trades_response = requests.get(trades_url)\n",
    "    trades_data = pd.DataFrame()\n",
    "    book_data = pd.DataFrame()\n",
    "\n",
    "    if trades_response.status_code == 200:\n",
    "        trades_data = pd.DataFrame(trades_response.json())\n",
    "    else:\n",
    "        print(f\"Error fetching trades: {trades_response.json()}\")\n",
    "\n",
    "    book_url = f'{base_url}/products/{product_id}/book?level=2'\n",
    "    book_response = requests.get(book_url)\n",
    "    if book_response.status_code == 200:\n",
    "        bids = pd.DataFrame(book_response.json()['bids'], columns=['price', 'size', 'num-orders'])\n",
    "        asks = pd.DataFrame(book_response.json()['asks'], columns=['price', 'size', 'num-orders'])\n",
    "        book_data = pd.concat([bids, asks], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"Error fetching order book: {book_response.json()}\")\n",
    "\n",
    "    return trades_data, book_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c21b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id = 'BTC-USD'\n",
    "start_date = pd.to_datetime('2022-01-01')\n",
    "end_date = pd.to_datetime('2024-03-22')\n",
    "##end_date = datetime.now()\n",
    "granularity = 3600  # 1 hour in seconds  # Daily data (60, 300, 900, 3600, 21600, 86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3e890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fetch_data_in_chunks(product_id, start_date, end_date, granularity)\n",
    "\n",
    "# Fetch market orders\n",
    "#trades, order_book = get_market_orders(product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d16860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 datetime      open      high       low     close       volume\n",
      "0     2022-01-13 12:00:00  43600.03  43833.73  43621.42  43820.28   339.543581\n",
      "1     2022-01-13 11:00:00  43530.00  43852.73  43805.43  43618.57   377.402679\n",
      "2     2022-01-13 10:00:00  43782.87  44043.69  43902.05  43805.42   248.686533\n",
      "3     2022-01-13 09:00:00  43767.81  44009.08  43920.02  43902.03   268.967343\n",
      "4     2022-01-13 08:00:00  43689.93  43924.39  43722.05  43920.02   220.509126\n",
      "...                   ...       ...       ...       ...       ...          ...\n",
      "19457 2024-03-11 04:00:00  68501.01  68838.04  68623.25  68577.58   217.306802\n",
      "19458 2024-03-11 03:00:00  68314.32  68660.40  68349.05  68623.02   252.320047\n",
      "19459 2024-03-11 02:00:00  68280.63  68559.04  68348.99  68354.81   398.627069\n",
      "19460 2024-03-11 01:00:00  67636.62  68428.06  68167.86  68348.99  1180.293303\n",
      "19461 2024-03-11 00:00:00  67112.21  69038.70  69032.80  68157.54  1316.015692\n",
      "\n",
      "[19462 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9791b5",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49d51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60f78fab",
   "metadata": {},
   "source": [
    "## Real-time data fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import websocket\n",
    "\n",
    "# Initialize an empty DataFrame to store real-time data\n",
    "real_time_data = pd.DataFrame(columns=['time', 'price', 'volume'])\n",
    "\n",
    "# Define the WebSocket URL\n",
    "socket = \"wss://ws-feed.pro.coinbase.com\"\n",
    "\n",
    "# Last update timestamp\n",
    "last_update = datetime.datetime.utcnow()\n",
    "\n",
    "def on_message(ws, message):\n",
    "    global real_time_data, last_update\n",
    "    \n",
    "    # Parse the incoming message\n",
    "    json_message = json.loads(message)\n",
    "    \n",
    "    # Process ticker messages only\n",
    "    if json_message['type'] == 'ticker':\n",
    "        # Convert timestamp to datetime\n",
    "        timestamp = pd.to_datetime(json_message['time'])\n",
    "        \n",
    "        # Check if a minute has passed since the last update\n",
    "        if (timestamp - last_update) >= datetime.timedelta(minutes=1):\n",
    "            # Update the last update timestamp\n",
    "            last_update = timestamp\n",
    "            \n",
    "            # Append the new data to the DataFrame\n",
    "            new_data = {\n",
    "                'time': timestamp,\n",
    "                'price': float(json_message['price']),\n",
    "                'volume': float(json_message['last_size']),\n",
    "            }\n",
    "            real_time_data = real_time_data.append(new_data, ignore_index=True)\n",
    "            \n",
    "            # Print the updated DataFrame\n",
    "            print(real_time_data.tail())  # Print the last few rows\n",
    "\n",
    "def on_error(ws, error):\n",
    "    print(error)\n",
    "    \n",
    "def on_close(ws):\n",
    "    print(\"WebSocket closed\")\n",
    "    \n",
    "def on_open(ws):\n",
    "    # Subscribe to the ticker channel\n",
    "    subscribe_message = {\n",
    "        \"type\": \"subscribe\",\n",
    "        \"channels\": [{\"name\": \"ticker\", \"product_ids\": [\"BTC-USD\"]}]\n",
    "    }\n",
    "    ws.send(json.dumps(subscribe_message))\n",
    "\n",
    "# Create and start the WebSocket client\n",
    "ws = websocket.WebSocketApp(socket,\n",
    "                            on_open=on_open,\n",
    "                            on_message=on_message,\n",
    "                            on_error=on_error,\n",
    "                            on_close=on_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c30d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the WebSocket client\n",
    "ws.run_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eff797",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce93458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"date\", \"close\", \"volume\", \"trades\"]]\n",
    "df[\"Ret\"] = df[\"close\"].pct_change()\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "del df[\"close\"]\n",
    "df = df.loc[(df[\"year\"] >= 2021)]  # To reduce the computational time\n",
    "df[\"Ret\"] = df[\"Ret\"].fillna(0)\n",
    "df[\"volume\"] = df[\"volume\"].fillna(0)\n",
    "df[\"trades\"] = df[\"trades\"].fillna(0)\n",
    "df = df.reindex(\n",
    "    columns=[\n",
    "        \"date\",\n",
    "        \"Ret\",\n",
    "        \"volume\",\n",
    "        \"trades\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"week\",\n",
    "        \"weekday\",\n",
    "        \"hour\",\n",
    "    ]\n",
    ")\n",
    "df = df[[\"date\", \"Ret\"]]\n",
    "\n",
    "df[\"Ret_10\"] = df[\"Ret\"].rolling(10).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret_50\"] = df[\"Ret\"].rolling(50).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "\n",
    "df[\"Ret_25\"] = df[\"Ret\"].rolling(25).apply(lambda x: np.prod(1 + x / 100) - 1)\n",
    "df[\"Ret25\"] = df[\"Ret_25\"].shift(-25)\n",
    "del df[\"Ret_25\"]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdf, ydf = df.iloc[:, 1:-1], df.iloc[:, -1]\n",
    "X = Xdf.astype(\"float32\")\n",
    "y = ydf.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16956f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.2\n",
    "train_split = 0.625\n",
    "train_size = int(len(df) * train_split)\n",
    "val_size = int(train_size * val_split)\n",
    "test_size = int(len(df) - train_size)\n",
    "\n",
    "window_size = 30\n",
    "\n",
    "ts = test_size\n",
    "split_time = len(df) - ts\n",
    "test_time = df.iloc[split_time + window_size :, 0:1].values\n",
    "\n",
    "\n",
    "y_train_set = y[:split_time]\n",
    "y_test_set = y[split_time:]\n",
    "\n",
    "X_train_set = X[:split_time]\n",
    "X_test_set = X[split_time:]\n",
    "\n",
    "n_features = X_train_set.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_input = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_input.fit(X_train_set)\n",
    "X_train_set_scaled = scaler_input.transform(X_train_set)\n",
    "X_test_set_scaled = scaler_input.transform(X_test_set)\n",
    "\n",
    "mean_ret = np.mean(y_train_set)\n",
    "\n",
    "scaler_output = MinMaxScaler(feature_range=(-1, 1))\n",
    "y_train_set = y_train_set.values.reshape(len(y_train_set), 1)\n",
    "y_test_set = y_test_set.values.reshape(len(y_test_set), 1)\n",
    "scaler_output.fit(y_train_set)\n",
    "y_train_set_scaled = scaler_output.transform(y_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b88e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time = df.iloc[:split_time, 0:1].values\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(window_size, y_train_set_scaled.shape[0]):\n",
    "    X_train.append(X_train_set_scaled[i - window_size : i, :])\n",
    "    y_train.append(y_train_set_scaled[i])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "print(\"Shape of training data\", X_train.shape, y_train.shape)\n",
    "\n",
    "X_test = []\n",
    "y_test = y_test_set\n",
    "\n",
    "for i in range(window_size, y_test_set.shape[0]):\n",
    "    X_test.append(X_test_set_scaled[i - window_size : i, :])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "print(\"Shape of test data\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02ca13",
   "metadata": {},
   "source": [
    "## Deep Learning for Estimating Fill Probabilities in a Limit Order Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ccca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=1, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b356988",
   "metadata": {},
   "source": [
    "## Reinforcement Learning for Optimal Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424619d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create an environment for trading strategy\n",
    "env = CryptoTradingEnv()\n",
    "\n",
    "# Define your Q-learning parameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.95\n",
    "exploration_rate = 1.0\n",
    "max_exploration_rate = 1.0\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Exploration-exploitation trade-off\n",
    "        if np.random.uniform(0, 1) < exploration_rate:\n",
    "            action = env.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])  # Exploit learned values\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-table\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_factor * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84130762",
   "metadata": {},
   "source": [
    "## Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "original_dim = x_train.shape[1]\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# Use reparameterization trick to ensure correct gradient\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "# VAE loss = mse_loss or xent_loss + kl_loss\n",
    "reconstruction_loss = mse(inputs, outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "\n",
    "# Train the autoencoder\n",
    "vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1907631",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load FinBERT model for sentiment analysis\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "# Define a function to scrape\n",
    "def scrape_news_sentiment(url):\n",
    "    # Make a request to the website\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Initialize BeautifulSoup object to parse HTML\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # Find all news articles on the page\n",
    "    articles = soup.find_all('article')\n",
    "    news_contents = []\n",
    "    sentiments = []\n",
    "    \n",
    "    # Iterate over each article, scrape the text, and perform sentiment analysis\n",
    "    for article in articles:\n",
    "        # Scrape the text from each article\n",
    "        text = article.get_text()\n",
    "        \n",
    "        # Use the nlp pipeline to perform sentiment analysis on the scraped text\n",
    "        sentiment = nlp(text)\n",
    "        \n",
    "        # Append the text and sentiment to the corresponding lists\n",
    "        news_contents.append(text)\n",
    "        sentiments.append(sentiment)\n",
    "    \n",
    "    return news_contents, sentiments\n",
    "\n",
    "# Define the URL of the site from which to scrape the news\n",
    "news_url = \"https://crypto.news/\"\n",
    "contents, analyzed_sentiments = scrape_news_sentiment(news_url)\n",
    "\n",
    "# Output the contents and sentiments\n",
    "for content, sentiment in zip(contents, analyzed_sentiments):\n",
    "    print(f\"Content: {content[:200]}\")  # Truncated for brevity\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf71c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
